\documentclass[a4paper, 12pt]{report}
\usepackage[utf8]{vietnam}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage[left=3.5cm,right=2cm,top=3.5cm,bottom=3cm]{geometry}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{float}
\usepackage{pdfpages}

\usepackage{eufrak}
\usepackage{listings}
\usepackage{tabu}
\renewcommand{\baselinestretch}{1.5}
\setlength{\parindent}{0pt}
\pagestyle{plain}
%\documentclass{article}
%\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{enumitem}

\usepackage[vietnamese=nohyphenation]{hyphsubst}
\usepackage[vietnamese]{babel}
%\setlength{\parindent}{1cm} % Default is 15pt.

\usepackage{titlesec}
\usepackage{subcaption}

\titleformat*{\section}{\LARGE\bfseries}
\titleformat*{\subsection}{\Large\bfseries}
\titleformat*{\subsubsection}{\large\bfseries}
\titleformat*{\paragraph}{\large\bfseries}
\titleformat*{\subparagraph}{\large\bfseries}

%\newtheorem{theorem}{Định lý}[section]

%\newtheorem*{theorem}{Định lý}[section]
%\newtheorem{theorem}{Theorem}
\usepackage{amsthm}

\newtheorem*{theorem}{Định lý } %without numbering
\newtheorem*{corollary}{Hệ quả } %without numbering
\newtheorem{theoremnum}{Định lý } %without numbering

% \newtheorem{theorem}{Theorem}  %with numberring

\newtheorem{Proposition}{Mệnh đề } %without numbering
\newtheorem*{Propositionproof}{Chứng minh mệnh đề 1: } %without numbering
\newtheorem{claim}{Khẳng định}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{}
\fancyhead[R]{\itshape Đồ án tốt nghiệp 20201}

\fancyfoot{}
\fancyfoot[L]{\itshape Thực hiện: Lai Đức Thắng}
\fancyfoot[R]{Toán Tin K61}
%\lfoot{\textit{Lai Đức Thắng}}
%
%\rfoot{\textit{Toán Tin K61}}

\renewcommand{\footrulewidth}{1,2pt}
%\usepackage[english]{babel}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
%\setlength{\parindent}{4em}

\begin{document}



	\begin{titlepage} % Đây là trang bìa


			\begin{center}
				
				{\large\bf TRƯỜNG ĐẠI HỌC BÁCH KHOA HÀ NỘI}\\
				
				{\large\bf VIỆN TOÁN ỨNG DỤNG VÀ TIN HỌC} \\
				
				{———————o0o——————–}
				\vskip 1cm
%			\includegraphics[scale=0.3]{Logo_Hust}
				\begin{center}
     				\includegraphics[scale=.85]{Images/biaDA.png}
				\end{center}
				\vskip 1cm
				
%				{\Large\bf \textbf{PHƯƠNG PHÁP\\ 
%						PHÂN TÍCH THÀNH PHẦN CHÍNH\\
%						(Principal Component Analysis - PCA )
%				}}

				{\Large\bf \textbf{HỌC SÂU
						VÀ ỨNG DỤNG TRONG\\ BÀI TOÁN ĐẾM CÂY\\
				}}
				\vskip 1cm
				{\bf ĐỒ ÁN TỐT NGHIỆP ĐẠI HỌC}
				
				
				{\bf {\it Chuyên ngành:} TOÁN TIN}\\
%				{\bf {\it Chuyên sâu:} }Xác suất rời rạc
				
				\vskip 1cm
				
				\begin{tabular}{r l}
					
					Giảng viên hướng dẫn:&{\bf TS.  LÊ HẢI HÀ  }\\[0.5cm]
					
					Họ và tên sinh viên:&{\bf LAI ĐỨC THẮNG}\\[0.5cm]
					
					Số hiệu sinh viên:&{\bf 20163830}\\[0.5cm]
					Lớp:&{\bf Toán Tin K61}
					
				\end{tabular}
				
				\vfill
				
				{\bf HÀ NỘI - 2020}
				
			\end{center}
			
	\end{titlepage}

\tableofcontents{}

\chapter*{Danh mục từ viết tắt}
\addcontentsline{toc}{chapter}{Danh mục từ viết tắt}


%\chapter*{Danh sách hình vẽ}
\addcontentsline{toc}{chapter}{Danh sách hình vẽ}
\listoffigures

\chapter*{Lời cảm ơn}
\addcontentsline{toc}{chapter}{Lời cảm ơn}	
Trước hết, em xin gửi lời cảm ơn sâu sắc tới TS. Lê Hải Hà, Viện Toán Ứng Dụng Và Tin Học, thầy là người đã hướng dẫn trực tiếp cho
em trong suốt quá trình thực hiện đồ án. Nhờ có sự tận tâm
của thầy, em đã tiếp thu được nhiều kiến thức trong xử lý ảnh ở một số lĩnh vực đặc thù; những kiến thức để thực hiện đồ án này nhưng thầy đã từng bước giúp em tìm hiểu, cho em rất nhiều
ý kiến, cũng như giúp hoàn thành các thủ tục liên quan, qua đó giúp em hoàn
thành đồ án tốt nghiệp.  \\
\hspace*{1cm}Em xin chân thành gửi lời cảm ơn đến công ty cổ phần Techainer,  đơn vị được viện gửi em đến thực tập  đã tạo mọi điều kiện thuận lợi giúp đỡ em
trong suốt quá trình 4 tháng thực tập và làm việc tại đây, giúp em có thêm những kiến thức chuyên sâu hơn về xử lý ảnh cũng như quy trình triển khai, xây dựng một dự án về trí tuệ nhân tạo trong thực tế  qua đó giúp
em hoàn thành được đồ án này. Em xin gửi lời cảm ơn đến các anh, các bạn
nghiên cứu sinh tại công ty đã hỗ trợ, chỉ dạy cho em trong suốt thời gian
làm việc tại đây.  \\
\hspace*{1cm}Cuối cùng, em xin gửi cảm ơn tới tất cả những thầy cô trong trường Đại học Bách Khoa Hà Nội nói chung và thầy cô tại Viện Toán Ứng Dụng Và Tin
Học nói riêng đã tận tình chỉ dạy và cung cấp những kiến thức quý báu cho
em trong suốt 5 năm qua.
\chapter*{Lời mở đầu}
\addcontentsline{toc}{chapter}{Lời mở đầu}
Việc sản xuất và phân phối loại cây trồng có giá trị kinh tế cao đòi hỏi phải có sự giám sát thường xuyên đối với theo chu kì và theo mùa vụ. Đây là một nhiệm vụ quan trọng đối với việc quản lý tài nguyên, rừng. Để đáp ứng nhu cầu ngày càng tăng của thế giới và sản lượng nông nghiệp tiêu dùng, năng suất cây trồng phải được ước tính. Việc phát hiện, thu thập thủ công dữ liệu cây trồng để lưu trữ hồ sơ trên một diễn tích đất lớn là không khả thi đối với con người, gây tốn kém tiền bạc và dễ bị ảnh hưởng bởi lỗi của con người
Việc phát hiện cây trồng trên một diện tích lớn sẽ giúp doanh nghiệp giảm bớt sự phụ thuộc vào con người và chi phí, dễ dàng quản lý và lên kế hoạch kịp thời cho sản xuất. Trong vài năm trở lại đây, lĩnh vực học sâu và xử lý ảnh đang nhận được nhiều sự quan tâm, áp dụng trong nhiều lĩnh vực khoa học khác nhau.  Hơn nữa,  cùng với sự phát triển của công nghệ,  các bức ảnh viễn thám chụp bề mặt trái đất có kích thước ngày càng lớn, có độ nét cao,  mang lại cơ hội cho việc viễn thám các cảnh quan sinh học có quy mô từ các sinh vật riêng lẻ đến các hệ thống toàn cầu. Trong báo cáo đồ án này, em xin giới thiệu một giải pháp phát hiện cây trong các bức ảnh viễn thám dựa trên mô hình học sâu. Nội dung của đồ án gồm có phần mở đầu, 4 chương, phần kết luận, tài liệu tham khảo:
\begin{itemize}
	\item \textbf{Chương 1: Lý thuyết học sâu và bài toán nhận diện vật thể}: Chương này trình bày về cơ sở lý thuyết của học sâu; giới thiệu chung về bài toán nhận diện vật thể và mô hình mạng nơ-ron nhân tạo sẽ được sử dụng phục vụ cho bài toán.
	\item \textbf{Chương 2: }

	\item \textbf{Chương 3: }

\end{itemize}

%\chapter{Lý thuyết về hệ thống thông tin địa lý và tiền xử lý ảnh viễn thám}
%
%\section{Hệ thống thông tin địa lý}
%
%\section{Ảnh viễn thám và quá trình tiền xử lý}




\chapter{Lý thuyết học sâu và bài toán nhận diện vật thể}
Deep Learning là một kỹ thuật Machine Learning mà ở đó huấn luyện máy tính giống như cách thức tự nhiên của con người: Học qua các ví dụ. Những năm gần đây, Deep learning đã mang đến nhiều bất ngờ trên quy mô toàn cầu và dẫn đường cho những tiến triển nhanh chóng trong nhiều lĩnh vực khác nhau như thị giác máy tính, xử lý ngôn ngữ tự nhiên (natural language processing), nhận dạng giọng nói tự động (automatic speech recognition), học tăng cường (reinforcement learning), và mô hình hoá thống kê (statistical modeling). Với những tiến bộ này, chúng ta bây giờ có thể xây dựng xe tự lái với mức độ tự động ngày càng cao (nhưng chưa nhiều tới mức như vài công ty đang tuyên bố), xây dựng các hệ thống giúp trả lời thư tự động khi con người ngập trong núi email, hay lập trình phần mềm chơi cờ vây có thể thắng cả nhà vô địch thế giới, một kỳ tích từng được xem là không thể đạt được trong nhiều thập kỷ tới. Những công cụ này đã và đang gây ảnh hưởng rộng rãi tới các ngành công nghiệp và đời sống xã hội, thay đổi cách tạo ra các bộ phim, cách chẩn đoán bệnh và đóng một vài trò ngày càng tăng trong các ngành khoa học cơ bản – từ vật lý thiên văn tới sinh học.\par
Với Deep Learning, một mô hình máy tính học cách thực hiện một công việc phân loại (classification) trực tiếp từ các hình ảnh, chữ viết (text) hoặc âm thanh. Các mô hình (models) Deep Learning có thể đạt được độ chính xác cao, đôi khi còn hơn cả con người. Các mô hình được huấn luyện bởi việc sử dụng một tập bao gồm bộ dữ liệu được gán nhãn và các kiến trúc mạng neural gồm nhiều lớp (layer). \par
\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{Images/aimldl}
	\caption{Mối quan hệ giữa AI, Machine Learning và Deep Learning.
		(Nguồn: \textit{What’s the Difference Between Artificial Intelligence, Machine Learning, and Deep Learning?})}
	\label{fig:aimldl}
\end{figure}
\textbf{Vậy điều gì mang đến sự thành công của deep learning?} Rất nhiều những ý tưởng cơ bản của deep learning được đặt nền móng từ những năm 80-90 của thế kỷ trước, tuy nhiên deep learning chỉ đột phá trong khoảng từ năm 2012. Vì sao?\par
Có thể kể đến một vài nhân tố dẫn đến sự bùng nổ này:
\begin{itemize}
	\item Sự ra đời của các bộ dữ liệu lớn được gán nhãn.
	\item Khả năng tính toán song song tốc độ cao của GPU.
	\item Sự cải tiến của các kiến trúc: GoogLeNet, VGG, ResNet, … và các kỹ thuật transfer learning, fine tuning.
	\item Nhiều thư viện mới hỗ trợ việc huấn luyện deep network với GPU: Theano, Caffe, TensorFlow, PyTorch, Keras,\dots
\end{itemize}

\section{Neural Network - Mạng Neural}
Tổng quan kiến trúc một mạng Neural như sau\par
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/nn1}
	\caption{Ví dụ một mạng Neural có $k$ tầng ẩn (Nguồn: \textit{CS229})}
	\label{fig:nn1}
\end{figure}
Với $i$ là lớp thứ $i$ của mạng, $j$ là đơn vị ẩn thứ $j$ của lớp, ta có:
$$ z_j^{[i]} = {w_j^{[i]}}^Tx + b_j^{[i]}$$
trong đó: $w$ là weight, $b$ là bias, $z$ là đầu ra. \par
\textbf{Hàm kích hoạt (Activation function):} Bản chất của công thức trên là một tổ hợp tuyến tính giữa các giá trị input $x$ và bộ trọng số $w$, do đó, khi áp dụng chúng với các dữ liệu mà có dạng tuyến tính, tức là dữ liệu mà ta có thể kẻ một đường thẳng để phân cách giữa chúng thì công thức tổ hợp trên đã đủ để giúp cho mô hình máy học có thể hoạt động tốt, chúng ta không cần tới hàm kích hoạt (activation function). Nhưng với các dữ liệu không có dạng tuyến tính, ta không thể kẻ một đường thẳng tuyến tính mà phân tách 2 dữ liệu ra được, và câu hỏi đặt ra là làm thế nào với một công thức tổ hợp tuyến tính như ban đầu mà dùng để phân lớp dữ liệu phi tuyến tính được. \textbf{Hàm kích hoạt} được tạo ra để làm điều này, hàm kích hoạt đóng vai trò như một người trung gian có nhiệm vụ chuyển đổi, nén hoặc chế biến output $z$ từ tuyến tính trở thành phi tuyến tính. \par
\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{Images/nn2}
	\caption{Một số hàm kích hoạt thường dùng (Nguồn: \textit{CS229})}
	\label{fig:nn2}
\end{figure}%\subsubsection{Logistic Regression}
%\textbf{Logistic Regression} là một mô hình thường được áp dụng cho các bài toán phân lớp nhị phân. Trong mô hình này, đầu ra có thể được thể hiện dưới dạng xác suất. Ví dụ, xác suất thi đỗ nếu biết thời gian ôn thi, xác suất ngày mai có mưa dựa trên những thông tin đo được trong ngày hôm nay,\dots Mô hình này có tên là logistic
%regression. Mặc dù trong tên có chứa từ \textit{regression}, logistic regression thường được sử dụng nhiều hơn cho các bài toán phân lớp.\par
%\begin{figure}[!h]
%	\centering
%%	\includegraphics[width=0.7\linewidth]{Images/logit}
%	\caption{Ví dụ về kết quả thi dựa trên số giờ ôn tập (Nguồn: \textit{Machine Learning cơ bản})}
%	\label{fig:logit}
%\end{figure}

\textbf{Hàm mất mát (Loss function):} Hàm mất mát trả về một số thực không âm thể hiện sự chênh lệch giữa hai đại lượng: $y_{pred}$ là giá trị được dự đoán và $y_{true}$ là giá trị thực. Trong trường hợp lý tưởng, $y_{pred} = y_{true}$, hàm mất mát sẽ có giá trị bằng $0$. Hàm loss được sử dụng phổ biến trong các mô hình Deep learning hiện nay là Cross-entropy cùng các biến thể cải tiến của nó (Weighted cross entropy, Focal loss...). Cross-entropy loss $L(z, y)$ được định nghĩa như sau:
$$ L(z, y) = - [ylog{z} + (1-y)log(1-z)]$$ 
\textbf{Optimizer và Learning rate:} Sau khi tính giá trị hàm loss, việc cần làm là tối ưu (cực tiểu hóa) hàm loss và update bộ trọng số $\{w\}$ mới. Learning rate, thường được ký hiệu là $\alpha$ hoặc $\eta$, thể hiện cho tốc độ học hay tốc độ update trọng số. Learning rate có thể là cố định hoặc được thay đổi tuỳ biến trong quá trình học. \par 
\textbf{Lan truyền ngược (Backpropagation):} Lan truyền ngược là phương thức dùng để cập nhật trọng số trong mạng neural bằng cách tính toán đầu ra thực sự và đầu ra mong muốn. Đạo hàm theo trọng số ww được tính bằng cách sử dụng quy tắc chuỗi (chain rule) dưới đây:
$$ \frac{\partial L(z, y)}{\partial w} = \frac{\partial L(z, y)}{\partial a} \times \frac{\partial a}{\partial z} \times \frac{\partial z}{\partial w} $$ 
Như kết quả, trọng số được cập nhật như sau:
$$ w := w - \eta\frac{\partial L(z, y)}{\partial w} $$ 
Tổng kết lại, ta có sơ đồ quá trình học của một mạng neural cơ bản như sau\par
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/nn3}
	\caption{Mối quan hệ giữa network, layers, loss function và optimizer (Nguồn: Deep Learing with Python - Francois Chollet)}
	\label{fig:nn3}
\end{figure}

\section{Mạng Neural tích chập (Convolutional Neural Network - CNN)}
Mạng Neural tích chập là một trong những mô hình Deep learning phổ biến nhất và có ảnh hưởng nhiều nhất trong lĩnh vực Computer Vision. CNNs được dùng trong trong nhiều bài toán như nhân dạng ảnh, phân tích video, ảnh MRI, hoặc có thể cho cả các bài của lĩnh vự xử lý ngôn ngữ tự nhiên, và hầu hết đều giải quyết tốt các bài toán này.\par
Mạng neural tích chập, còn được biết đến với tên CNNs, là một dạng mạng neural được cấu thành bởi các layer sau:\par
\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{Images/cnn1}
	\caption{Ví dụ về một CNN (Nguồn: \textit{CS230})}
	\label{fig:nn3}
\end{figure}
\textbf{Lớp tích chập (Convolution layer):} Tầng tích chập (CONV) sử dụng các bộ lọc (filters) để thực hiện phép tích chập khi đưa chúng đi qua input $I$ theo các chiều của nó. Các \textit{hyperparameters} của filter bao gồm  kích thước $F$, độ trượt (stride) $S$. Kết quả đầu ra của lớp này được gọi là feature map.\par
\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{Images/cnn2}
	\caption{Mô tả hoạt động của CONV (Nguồn: \textit{CS230})}
	\label{fig:cnn2}
\end{figure}
\textbf{Stride} là số lượng pixel dịch chuyển trên ma trận đầu vào hay Stride dùng để dịch chuyển filter theo mỗi bước xác định.\par
\begin{center}
\begin{minipage}{4cm}
	\centering
	\includegraphics[scale=0.5]{Images/stride1}
\end{minipage}
\begin{minipage}{4cm}
	\centering
	\includegraphics[scale=0.425]{Images/stride2}
\end{minipage}
\begin{minipage}{4cm}
	\centering
	\includegraphics[scale=0.5]{Images/stride3}
\end{minipage}

Ví dụ về stride = 1 và stride bằng 2
\end{center}
\textbf{Padding:} Khi áp dụng phép CONV thì ma trận đầu vào sẽ có nhỏ dần đi, do đó số layer của mô hình CNN sẽ bị giới hạn, và không thể xậy đựng mô hình mong muốn. Để giải quyết tình trạng này, ta cần "bọc" bên ngoài ma trận đầu vào để đảm bảo kích thước đầu ra sau mỗi tầng convolution là không đổi. Do đó có thể xậy dựng được mô hình với số tầng convolution lớn tùy ý. Một cách đơn giản và phổ biến nhất để padding là sử dụng hàng số 0, ngoài ra có một số phương pháp khác như reflection padding hay là symmetric padding. \par
\textbf{Lớp Pooling:} Pooling layer thường được dùng giữa các convolutional layer, để giảm kích thước dữ liệu nhưng vẫn giữ được các thuộc tính quan trọng. Kích thước dữ liệu giảm giúp giảm việc tính toán trong model.\par
\textbf{Spatial pooling} được gọi là lấy mẫu con làm giảm chiều của mỗi map nhưng vẫn giữ được thông tin quan trọng. Spatial pooling có thể có nhiều loại khác nhau như Max Pooling và Average Pooling. \par
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.9\linewidth]{Images/pooling}
	\caption{Hai kiểu pooling phổ biến (Nguồn: \textit{CS230})}
	\label{fig:pooling}
\end{figure}
\textbf{Fully Connected (FC):} Lớp Fully Connected nhận đầu vào là các dữ liệu đã được làm phẳng, mà mỗi đầu vào đó được kết nối đến tất cả neural. Trong mô hình mạng CNNs, các lớp FC thường được tìm thấy ở cuối mạng và được dùng để tối ưu hóa mục tiêu của mạng ví dụ như độ chính xác của lớp. \par
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.9\linewidth]{Images/fc}
	\caption{Fully Connected layer (Nguồn: \textit{CS230})}
	\label{fig:fc}
\end{figure}
\textbf{Các hàm kích hoạt thường gặp:} 
\begin{itemize}
	\item \textbf{Rectified Linear Unit (ReLU)}: ReLU là một hàm kích hoạt $g$ được sử dụng trên tất cả các thành phần. Mục đích của nó là tăng tính phi tuyến tính cho mạng. Những biến thể khác của ReLU được tổng hợp ở bảng dưới\par
	\begin{figure}[h!]
		\centering
		\includegraphics[width=1\linewidth]{Images/relu1}
		\caption{ReLU và biến thể (Nguồn: \textit{CS230})}
		\label{fig:relu1}
	\end{figure}
	\item \textbf{Softmax:} Bước softmax có thể được coi là một hàm logistic tổng quát lấy đầu vào là một vector chứa các giá trị $x \in \mathbb{R}^n$ và cho ra là một vector gồm các xác suất $p \in \mathbb{R}^n$ thông qua một hàm softmax ở layer cuối.
	$$ p = (p_1,\dots,p_n)^T \text{ trong đó } p = \frac{e^{x_i}}{\displaystyle\sum_{j=1}^ne^{x_j}} $$
\end{itemize}

\section{Bài toán nhận diện vật thể (Object Detection)}
\subsection{Tổng quan bài toán}
Các hình ảnh trong cuộc sống bình thường thì không chỉ chứa 1 đối tượng mà thường bao gồm rất nhiều các đối tượng. Ta quan tâm đến vị trí của từng đối tượng trong ảnh. Bài toán như vậy được gọi là: object detection.\par
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{Images/det}
	\caption{Ví dụ về đầu ra của bài toán nhận diện vật thể}
	\label{fig:det}
\end{figure}
Bài toán object detection có input là ảnh màu và output là vị trí của các đối tượng trong ảnh. Ta thấy nó bao gồm 2 bài toán nhỏ:
\begin{itemize}
	\item Xác định các bounding box (hình chữ nhật) quanh đối tượng.
	\item Với mỗi bouding box thì cần phân loại xem đấy là đối tượng gì (người, mèo, ô tô,…) với bao nhiêu phần trăm chắc chắn.
\end{itemize}
Object Detection là 1 bài toán đã đạt được rất nhiều các thành tựu trong những năm gần đây, cả phần ứng dụng và mô hình thuật toán. Điển hình là các phương pháp Object Detection sử dụng Deep Learning đã đạt được các bước cải thiện vượt trội so với các phương pháp xử lý ảnh thông thường khác.\par
Có hai loại bài toán Object detection: two-stage object detection và one-stage object detection.\par
\textbf{Two-stage object detection:} Điển hình họ các thuật toán R-CNN. Việc gọi là two-stage là do cách model xử lý để lấy ra được các vùng có khả năng chứa vật thể từ bức ảnh. Ví dụ, với Faster-RCNN thì trong stage-1, ảnh sẽ được đưa ra 1 sub-network gọi là RPN (Region Proposal Network) với nhiệm vụ extract các vùng trên ảnh có khả năng chứa đối tượng dựa vào các anchor. Sau khi đã thu được các vùng đặc trưng từ RPN, model Faster-RCNN sẽ thực hiện tiếp việc phân loại đối tượng và xác định vị trí nhờ vào việc chia làm 2 nhánh tại phần cuối của mô hình (Object classification \& Bounding box regression).\par
\textbf{One-stage Object Detection:} Các thuật toán điển hình như: SSD, YOLO, RetinaNet. Gọi là one-stage vì trong việc thiết kế model hoàn toàn không có phần trích chọn các vùng đặc trưng (các vùng có khả năng chứa đối tượng) như RPN của Faster-RCNN. Các mô hình one-stage object detection coi phần việc phát hiện đối tượng (object localization) như một bài toán regression (với 4 tọa độ offset, ví dụ x, y, w, h) và cũng dựa trên các box được định nghĩa sẵn gọi là anchor để làm việc đó. Các mô hình dạng này thường nhanh hơn tuy nhiên "độ chính xác" của model thường kém hơn so với two-stage object detection. Tuy nhiên, một số mô hình one-stage vẫn tỏ ra vượt trội hơn một chút so với two-stage như Retina-Net với việc việc thiết kế mạng theo FPN (Feature Pyramid Network) và Focal Loss.\par

\subsection{Sơ lược về Transfer Learning}
Những năm gần đây, Deep Learning phát triển cực nhanh dựa trên lượng dữ liệu training khổng lồ và khả năng tính toán ngày càng được cải tiến của các máy tính. Các kết quả cho bài toán phân loại ảnh ngày càng được nâng cao. Bộ cơ sở dữ liệu thường được dùng nhất là ImageNet với 1.2M ảnh cho 1000 classes khác nhau. Rất nhiều các mô hình Deep Learning đã giành chiến thắng trong các cuộc thi ILSVRC (ImageNet Large Scale Visual Recognition Challenge). Có thể kể ra một vài: AlexNet, ZFNet, GoogLeNet, ResNet, VGG.\par
Nhìn chung, các mô hình này đều bao gồm rất nhiều layers. Các layers phía trước thường là các Convolutional layers kết hợp với các nonlinear activation functions và pooling layers (và được gọi chung là ConvNet). Layer cuối cùng là một Fully Connected Layer và thường là một Softmax Regression (Xem Hình 1). Số lượng units ở layer cuối cùng bằng với số lượng classes (với ImageNet là 1000). Vì vậy output ở layer gần cuối cùng (second to last layer) có thể được coi là feature vectors và Softmax Regression chính là Classifier được sử dụng.\par
Chính nhờ việc features và classifier được trained cùng nhau qua deep networks khiến cho các mô hình này đạt kết quả tốt. Tuy nhiên, những mô hình này đều là các Deep Networks với rất nhiều layers. Việc training dựa trên 1.2M bức ảnh của ImageNet cũng tốn rất nhiều thời gian (2-3 tuần).\par
Với các bài toàn dựa trên tập dữ liệu khác, rất ít khi người ta xây dựng và train lại toàn bộ Network từ đầu, bởi vì có rất ít các cơ sở dữ liệu có kích thước lớn. Thay vào đó, phương pháp thường được dùng là sử dụng các mô hình (nêu phía trên) đã được trained từ trước, và sử dụng một vài kỹ thuật khác để giải quyết bài toán. Phương pháp sử dụng các mô hình có sẵn như thế này được gọi là Transfer Learning.\par
Có 2 loại transfer learning:
\begin{itemize}
	\item \textbf{Feature extractor:} Sau khi lấy ra các đặc điểm của ảnh bằng việc sử dụng ConvNet của pre-trained model, thì ta sẽ dùng linear classifier (linear SVM, softmax classifier,..) để phân loại ảnh.
	\item \textbf{Fine tuning:} Sau khi lấy ra các đặc điểm của ảnh bằng việc sử dụng ConvNet của pre-trained model, thì ta sẽ coi đây là input của 1 CNN mới bằng cách thêm các ConvNet và Fully Connected layer. 
\end{itemize}

\subsection{Một số phương pháp đánh giá mô hình nhận diện vật thể}
\subsubsection{Intersection over Union (IoU)}
Intersection over Union là chỉ số đánh giá được sử dụng để đo độ chính xác của Object detector trên tập dữ liệu cụ thể. IoU đơn giản chỉ là một chỉ số đánh giá. Mọi thuật toán có khả năng predict ra các bounding box làm output đều có thể được đánh giá thông qua IoU. \par
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\linewidth]{Images/iou1}
	\caption{Một ví dụ về phát hiện biển báo Stop từ hình ảnh. (Nguồn: \textit{pyimagesearch.com})}
	\label{fig:iou1}
\end{figure}
Để áp dụng được IoU để đánh giá một mô hình nhận diện vật thể bất kì ta cần:
\begin{itemize}
	\item Những ground-truth bounding box (bounding box đúng của đối tượng, ví dụ như bounding box của đối tượng được khoanh vùng và gán nhãn bằng tay sử dụng trong tập test.)
	\item Những bounding box dự đoán được model sinh ra.
\end{itemize}
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\linewidth]{Images/iou3}
	\caption{Tính toán Intersection over Union. (Nguồn: \textit{pyimagesearch.com})}
	\label{fig:iou3}
\end{figure}
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{Images/iou4}
	\caption{Một ví dụ về tính toán IoU cho những bounding box khác nhau. (Nguồn: \textit{pyimagesearch.com})}
	\label{fig:iou4}
\end{figure}

\subsubsection{Average Precision}
AP (Average Precision) là một metric phổ biến trong việc đánh giá độ chính xác của các mô hình nhận diện vật thể như Faster R-CNN, SSD,\dots\par	
\textbf{Precison $-$ Recall}\\
Với bài toán phân loại mà tập dữ liệu của các lớp là chênh lệch nhau rất nhiều, có một phép đó hiệu quả thường được sử dụng là Precision-Recall.\par
Trước hết xét bài toán phân loại nhị phân. Ta cũng coi một trong hai lớp là positive, lớp còn lại là negative.\par
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{Images/PR}
	\caption{Cách tính Precision và Recall. (Nguồn: \textit{Machine Learning cơ bản})}
	\label{fig:iou2}
\end{figure}
\textbf{Precision} cao đồng nghĩa với việc độ chính xác của các điểm tìm được là cao. \textbf{Recall} cao đồng nghĩa với việc True Positive Rate cao, tức tỉ lệ bỏ sót các điểm thực sự positive là thấp.\par
\textbf{Precision-Recall curve và Average precision}\\
Ta có thể đánh giá mô hình dựa trên việc thay đổi một ngưỡng và quan sát giá trị của Precision và Recall.\par
\textbf{Average Precision:} Ta xét một ví dụ, với bộ dữ liệu có 5 quả táo, mô hình lần lượt đưa ra 10 dự đoán, ta chọn ngưỡng cho dự đoán đúng là IoU > 0.5, dự đoán được xếp giảm dần theo sự 'tự tin' của dự đoán. Số liệu được thể hiện trong bảng sau: \par
	\begin{center}
		\begin{tabular}{||c c c c ||} 
			\hline
			Rank & Correct? & Precision & Recall \\ [0.5ex] 
			\hline\hline
			1 & True & 1.0 & 0.2   \\ 
			\hline
			2 & True & 1.0 & 0.4   \\
			\hline
			3 & False & 0.67 & 0.4 \\
			\hline
			4 & False & 0.5 & 0.4  \\
			\hline
			5 & False & 0.4 & 0.4  \\ 
			\hline
			6 & True & 0.5 & 0.6  \\
			\hline
			7 & True & 0.57 & 0.8  \\
			\hline
			8 & False & 0.5 & 0.8  \\
			\hline
			9 & False & 0.44 & 0.8  \\
			\hline
			10 & True & 0.5 & 1.0 \\
			\hline
		\end{tabular}
	\end{center}
Biểu diễn các điểm precision-recall ta được một đường zig-zag sau\par 
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{Images/screensprhot001}
	\caption{Precision-recall curve (Nguồn: \textit{Medium})}
	\label{fig:screensprhot001}
\end{figure}
Định nghĩa tổng quát cho Average Precision (AP) là diện tích phía dưới precision-recall curve 
$$ \text{AP} = \int_{0}^{1}p(r)dr$$
Precision và recall luôn nằm trong đoạn [0; 1] do đó AP cũng nằm trong đoạn [0; 1].  \par
Mean average precision (mAP) là trung bình của AP. Trong một số trường hợp, ta tính AP cho mỗi class và lấy trung bình của chúng, một số khác thì lại giống nhau. Ví dụ theo COCO, không có sự khác biệt giữa AP và mAP.\par
\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{Images/cocometric}
	\caption{Một số metric được sử dụng để đánh giá kết quả trên bộ dữ liệu COCO (Nguồn: \textit{cocodataset.org})}
	\label{fig:cocometric}
\end{figure}

\section{Mô hình RetinaNet}
%\subsection{Sơ lược về Transfer Learning}
%Những năm gần đây, Deep Learning phát triển cực nhanh dựa trên lượng dữ liệu training khổng lồ và khả năng tính toán ngày càng được cải tiến của các máy tính. Các kết quả cho bài toán phân loại ảnh ngày càng được nâng cao. Bộ cơ sở dữ liệu thường được dùng nhất là ImageNet với 1.2M ảnh cho 1000 classes khác nhau. Rất nhiều các mô hình Deep Learning đã giành chiến thắng trong các cuộc thi ILSVRC (ImageNet Large Scale Visual Recognition Challenge). Có thể kể ra một vài: AlexNet, ZFNet, GoogLeNet, ResNet, VGG.\par
%Nhìn chung, các mô hình này đều bao gồm rất nhiều layers. Các layers phía trước thường là các Convolutional layers kết hợp với các nonlinear activation functions và pooling layers (và được gọi chung là ConvNet). Layer cuối cùng là một Fully Connected Layer và thường là một Softmax Regression (Xem Hình 1). Số lượng units ở layer cuối cùng bằng với số lượng classes (với ImageNet là 1000). Vì vậy output ở layer gần cuối cùng (second to last layer) có thể được coi là feature vectors và Softmax Regression chính là Classifier được sử dụng.\par
%Chính nhờ việc features và classifier được trained cùng nhau qua deep networks khiến cho các mô hình này đạt kết quả tốt. Tuy nhiên, những mô hình này đều là các Deep Networks với rất nhiều layers. Việc training dựa trên 1.2M bức ảnh của ImageNet cũng tốn rất nhiều thời gian (2-3 tuần).\par
%Với các bài toàn dựa trên tập dữ liệu khác, rất ít khi người ta xây dựng và train lại toàn bộ Network từ đầu, bởi vì có rất ít các cơ sở dữ liệu có kích thước lớn. Thay vào đó, phương pháp thường được dùng là sử dụng các mô hình (nêu phía trên) đã được trained từ trước, và sử dụng một vài kỹ thuật khác để giải quyết bài toán. Phương pháp sử dụng các mô hình có sẵn như thế này được gọi là Transfer Learning.\par
%Có 2 loại transfer learning:
%\begin{itemize}
%	\item \textbf{Feature extractor:} Sau khi lấy ra các đặc điểm của ảnh bằng việc sử dụng ConvNet của pre-trained model, thì ta sẽ dùng linear classifier (linear SVM, softmax classifier,..) để phân loại ảnh.
%	\item \textbf{Fine tuning:} Sau khi lấy ra các đặc điểm của ảnh bằng việc sử dụng ConvNet của pre-trained model, thì ta sẽ coi đây là input của 1 CNN mới bằng cách thêm các ConvNet và Fully Connected layer. 
%\end{itemize}
%\subsubsection{PyTorch}
%PyTorch là một thư viện machine learning mã nguồn mở dựa trên Torch, được sử dụng cho lĩnh vực Thị giác máy tính (Computer Vision) và xử lý ngôn ngữ tự nhiên (Natural language processing), được phát triển bởi Phòng nghiên cứu AI của Facebook (FAIR). Pytorch tập trung vào 2 khả năng chính:
%\begin{itemize}
%	\item Một sự thay thế cho bộ thư viện numpy để tận dụng sức mạnh tính toán của GPU.
%	\item Một platform Deep learning phục vụ trong nghiên cứu, mang lại sự linh hoạt và tốc độ.
%\end{itemize}
%Cấu trúc dữ liệu cốt lõi được sử dụng trong PyTorch là \textbf{Tensor}.\par

%\subsection{Kiến trúc mô hình RetinaNet}
RetinaNet là một mạng tổng hợp bao gồm:
\begin{itemize}
	\item Mạng kim tự tháp đặc trưng-Feature Pyramid Networks (FPN) được xây dựng dưa trên mạng ResNet; chịu trách nhiệm tính toán,  trích xuất các đặc trưng trong bức ảnh
	\item Mạng con chịu trách nhiệm thực hiện phân loại đối tượng bằng cách sử dụng đầu ra của mạng kim tự tháp đặc trưng FPN
	\item Mạng con chịu trách nhiệm điều chỉnh các kích thước phát hiện vật thể từ đầu ra của mạng kim tự tháp đặc trưng FPN
\end{itemize}
\begin{figure}[!h]
	\centering
	\includegraphics[width=12cm,height=6cm]{Images/resnet_visulize}
	\caption{Kiến trúc mô hình RetinaNet.}
	\label{fig:resnet-fpn4}
\end{figure}

\subsection{Mạng kim tự tháp đặc trưng Feature Pyramid Network (FPN)}

\textbf{Residual Network (ResNet):} Một vấn đề phổ biến của Deep learning là Vanishing Gradients, tức là theo công thức tính đạo hàm bằng chain rule (trong lan truyền ngược, back propagation), nếu gradient của các lớp sau nhỏ thì gradient ở các lớp đầu sẽ gần bằng 0. Vì thế mà parameter của các lớp trước sẽ không được cập nhật nên các parameter này không đóng góp được gì trong việc đưa ra output. Vậy nên dù ta dùng nhiều lớp, thực chất số lớp hữu ích chỉ là một vài lớp cuối, điều đó làm giảm sự hiệu quả của mạng neural. ResNet ra đời để giải quyết vấn đề đó, giải pháp mà ResNet đưa ra là sử dụng kết nối "tắt" đồng nhất để xuyên qua một hay nhiều lớp. Một khối như vậy được gọi là một Residual Block.\par
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\linewidth]{Images/resnet_1}
	\caption{Khối thường (trái) và khối residual (phải)}
	\label{fig:resnet-1}
\end{figure}
ResNet-50 là một mạng bao gồm 50 lớp residual.\par

\textbf{Mạng kim tự tháp đặc trưng-Feature Pyramid Networks (FPN)}  \\
Phát hiện các đối tượng có kích thước nhỏ là một vấn đề vô cùng quan
trọng để nâng cao độ chính xác. Và FPN là mô hình mạng được thiết kế
ra dựa trên khái niệm kim tự tháp (pyramid) để giải quyết vấn đề này. \\
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\linewidth]{Images/fpn1}
	\caption{Feature Pyramid Network (FPN)}
	\label{fig:resnet-fpn1}
\end{figure}
\hspace*{1cm}Mô hình FPN giúp nhận diện được cả những đối tượng có kích thước
to và nhỏ nhờ sự kết hợp của tất cả các lớp kim tự tháp. Mô hình được
xây dựng bằng cách thực hiện quá trình bottom-up kết hợp với top-down
để dò tìm đối tượng (trong khi đó, các thuật toán khác chỉ thường sử dụng
bottom-up).Trong đó bottom-up sử dụng mạng tích chập thông thường
dùng để trích xuất các đặc trưng, khi chúng ta ở bottom và đi lên (up) độ
phân giải sẽ giảm, nhưng giá trị cấu trúc cấp cao ( high-level semantic) sẽ
tăng lên. \\
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\linewidth]{Images/fpn2}
	\caption{Trích xuất đặc trưng trong mô hình FPN}
	\label{fig:resnet-fpn2}
\end{figure}
%ảnh
\hspace*{1cm}Một vài các mô hình khác trong phát hiện đối tượng (ví dụ: SSD,..)
quyết định dựa vào nhiều feature map. Nhưng tầng ở dưới không được
sử dụng để nhận dạng đối tượng. Vì những tầng này có độ phân giải cao
nhưng giá trị cấu trúc cấp cao của chúng lại không đủ cao nên những nhà
nghiên cứu bỏ chúng đi để tăng tốc độ xử lý. Các nhà nghiên cứu biện
minh rằng các tầng ở dưới chưa đủ mức ý nghĩa cần thiết để nâng cao độ
chính xác, thêm các tầng đó vào sẽ không nâng độ chính xác cao thêm bao
nhiêu và họ bỏ chúng đi để có tốc độ tốt hơn. Cho nên, những mô hình phát hiện đối tượng này chỉ sử dụng các tầng ở lớp trên, và do đó sẽ không nhận
dạng được các đối tượng có kích thước nhỏ. \\
\hspace*{1cm}Trong khi đó, FPN xây dựng thêm mô hình top-down, nhằm mục đích
xây dựng các tầng có độ phân giải cao hơn từ các tầng high-level semantics \\
%ảnh 
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\linewidth]{Images/fpn3}
	\caption{Top-down}
	\label{fig:resnet-fpn3}
\end{figure}
\hspace*{1cm}Trong quá trình xây dựng lại các tầng top-down, chúng ta sẽ gặp một
vấn đề khá nghiêm trọng là bị mất mát thông tin của các đối tượng. Ví dụ
một đối tượng nhỏ khi đi lên (top) sẽ không thấy nó, và từ trên đi ngược
lại sẽ không thể tái tạo lại đối tượng nhỏ đó. Để giải quyết vấn đề này,
chúng ta sẽ tạo các kết nối (skip connection) giữa các reconstruction layer
và các feature maps để giúp quá trình dự đoán các vị trí của đối tượng
thực hiện tốt hơn (sao cho hàm mất mát đạt giá trị nhỏ nhất).  \\
\hspace*{1cm}Đồ hình bên dưới biểu diễn chi tiết đường đi theo bottom-up và topdown. P2, P3, P4, P5 là các pyramid của các feature map. \\
%ảnh
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\linewidth]{Images/fpn4}
	\caption{Bottom-up và top-down.}
	\label{fig:resnet-fpn4}
\end{figure}
\hspace*{1cm}FPN không phải là mô hình phát hiện đối tượng. Nó là mô hình phát
hiện đặc trưng và được sử dụng trong phát hiện đối tượng. Các feature
map từ P2 đến P5 trong Hình \ref{fig:resnet-fpn4} độc lập với nhau và các đặc trưng
được sử dụng để phát hiện đối tượng.
\subsection{Mạng con phân loại}
Mạng con phân loại là một mạng tích chập đầy đủ (FCN) gắn ở mỗi mức của FPN. Mạng con này chứa 4 lớp tích chập 3x3 với 256 bộ lọc, kết hợp với hàm kích hoạt RELU; tiếp đến là lớp tích chập 3x3 với số bộ lọc $= KA$. Do đó đầu ra của feature map có kích thước $W*H*KA$, trong đó $W,H$ là các kích thước của feature map, K,A lần lượt là số lượng class và số lượng anchor box. Lý do cho việc sử dụng lớp tích chập cuối cùng của mạng con phân loại có $K*A$  bộ lọc vì nếu có “A” anchor box được đề xuất cho mỗi vị trí trong feature map thu được từ lớp tích chập cuối thì mỗi anchor box  có khả năng được phân loại được $K$ lớp .  Do đó, đầu ra của feature map sẽ có $K*A$ bộ lọc.

\subsection{Mạng con hồi quy}
Song song với mạng con phân loại đối tượng, mô hình đi kèm một mạng tích chập đầy đủ khác vào mỗi cấp của mạng FPN nhằm mục đích điều chỉnh kích thước các anchor box trong việc  được phát hiện các đối tượng (nếu có).  Cấu trúc của mạng con này giống với mạng con phân loại ở trên, ngoại trừ lớp tích chập cuối có kích thước $3x3$ với $4$ bộ lọc cho kết quả đầu ra feature map của mạng con này có kích thước $W*H*4A$.  Lý do để lớp tích chập cuối có $4$ bộ lọc là vì để xác định vị trí đối tượng trong ảnh, mạng con hồi quy tạo ra $4$ giá trị cho mỗi anchor box  dự đoán độ lệch tương đối (về tọa độ tâm, chiều rộng và chiều cao) giữa các anchor box với  box thực tế chứa đối tượng. Do đó, đầu ra của feature map  trong mạng con hồi quy sẽ có $4*A$ bộ lọc (kênh)


\subsection{Hàm mất mát}
\hspace*{1cm}Trong bài toán nhận diện vật thể, nhãn dữ liệu mô tả chính xác vị trí đối tượng trong bức ảnh được gọi là các ground-truth boxes. Thông qua 2 mạng con ở trên, mỗi anchor box được dự đoán sẽ được phân vào một lớp nào đó và vị trí của anchor box đó trong ảnh. Để tính toán hàm mất mát trong training, ta cần phải so sánh các giá trị dự đoán này với nhãn của dữ liệu.  Một anchor box được coi là dự đoán đúng với một ground truth box nào đó nếu giá trị IOU giữa 2 đối tượng này > 0.5; một anchor box được coi là không khớp với bất kì ground truth box nào trong ảnh nếu giá trị IOU giữa chúng < 0.4.  CUối cùng, nếu giá trị IOU giữa một anchor box với bất kì ground truth box nào nằm giữa 0.4 và 0.5 sẽ không đóng góp vào hàm mất mát. \\
\hspace*{1cm}
Hàm mất mát của mô hình RetinaNet chứa 2 thành phần: Hàm mất mát hồi quy và hàm mất mát phân loại

\paragraph{Hàm mất mát hồi quy} \leavevmode \\

Giả sử các cặp dữ liệu trùng khớp nhau kí hiệu ${(A^i, G^i)}_{i=1,...N}$, trong đó $A$  là tập các anchor box, $G$ là tập các ground truth box, N là số lượng các ground truth box.

Với mỗi anchor, mô hình dự đoán 4 giá trị, kì hiệu lần lượt $P^i = (P^i_x, P^i_y, P^i_w, P^i_h)$. Hai giá trị đầu là độ lệch giữa tâm của anchor $A_{i}$ và ground truth $G_i$, trong đó 2 giá trị cuối là độ lệch giữa chiều cao và chiều rộng của 2 box đó. Tương ứng vỡi mỗi dự đoán, giá trị mục tiêu Ti được tính như là độ lệch giữa anchor box và gt box theo công thức:
\begin{align}
T^i_x &= (G^i_x - A^i_x) / A^i_w \quad & \\
T^i_y &= (G^i_y - A^i_y) / A^i_h & \\
T^i_w &= log(G^i_w / A^i_w) & \\
T^i_h &= log(G^i_h / A^i_h) &
\end{align}

Với các giá trị trên, hàm mất mát hồi quy được đĩnh nghĩa là:
\begin{equation}
	L_{loc} = \sum_{j \in \{x, y, w, h\}}smooth_{L1}(P^i_j - T^i_j) \quad
\end{equation}
trong đó hàm $smooth_{L1}(x)$ có công thức:
\begin{equation}
smooth_{L1}(x) = 
\begin{cases}
0.5x^2 &|x| < 1 \\
|x| - 0.5 &|x| \geq 1
\end{cases}
\end{equation}

\paragraph{Hàm mất mát phân loại} \leavevmode \\

Hàm mất mát phân loại mà mô hình retinanet sử dụng là Focal Loss. Nói một cách đơn giản, Focal loss (FL) là một phiên bản cải tiến của hàm Cross-Entropy Loss (CE) cố gắng xử lý vấn đề mất cân bằng giữa các lớp foreground và background; một điều thường xảy ra trong các bài toàn object detection .Hàm được xây dựng bằng cách gán nhiều trọng số hơn cho các mẫu khó hoặc dễ bị phân loại sai và giảm trọng số cho các mẫu dễ phân loại. Vì vậy, Focal loss làm giảm sự đóng góp tổn thất từ các mẫu dễ phân loại và tăng tầm quan trọng của việc sửa lỗi các mẫu được phân loại sai

Đầu tiên ta sẽ tìm hiểu về hàm Cross-Entropy (CE) loss trong bài toán phân loại nhị phân:

\begin{equation}
CE(p,y) = 
\begin{cases} 
-log(p) \textrm{     if      }  y =1 \\
-log(1-p) \textrm{     otherwise,      }
\end{cases}
\end{equation}
với $y$ là nhãn phân loại của dữ liệu,  $p \in [0, 1]$ là xác suất dự đoán cho lớp có nhãn y của mô hình. Để thuận tiện, ta định nghĩa $p_t$:
\begin{equation}
p_t = 
\begin{cases} 
p \textrm{    nếu      }  y =1 \\
1-p \textrm{     otherwise,      }
\end{cases}
\end{equation}
Khi đó $CE(p,y)=CE(p_t)=-log(p_t)$ \\
%FocalLoss_6_1
\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{Images/FocalLoss_6_1}
	\caption{Đồ thị hàm focal loss với các giá trị $\gamma$ khác nhau}
	\label{fig:focal-loss_gamma}
\end{figure} 

Đồ thị của CE loss có thể được xem là đường cong màu xanh trong hình \ref{fig:focal-loss_gamma} Một điểm đáng chú ý có thể dễ dàng nhận ra từ đồ thị trên, đó là ngay cả những mẫu dễ phân loại  (có $p_{t} >> 5$) cũng có giá trị loss khá lớn.\\
Trong cross entropy ta thấy rằng vai trò đóng góp vào loss function của các class cùng bằng $-\log(p_i)$. Khi xảy ra hiện tượng mất cân bằng, chúng ta muốn rằng mô hình sẽ dự báo chuẩn hơn đối với những class thiểu số. Do đó cần một hàm loss function hiệu quả hơn, có thể điều chỉnh được giá trị phạt lớn hơn đối với nhóm thiểu số. \\
Một kĩ thuật thường dùng để giải quyết việc mất cân bằng giữa các lớp là áp dụng trọng số $\alpha$ bằng nghịch đảo tần suất nhãn vào hàm cross entropy. Hàm loss function mới được gọi là balanced cross entropy:
\begin{equation}
BCE(\mathbf{p}, \mathbf{q}) = -\alpha_i\log(q_i),~~~ \text{với} ~ p_i=1
\end{equation}

hàm mất mát này là một mở rộng nhỏ của CE loss mà tác giả của bài báo coi là cơ sở cho đề xuất hàm mất mát mới: Focal loss. Focal loss là hàm loss function lần đầu được giới thiệu trong RetinaNet. Hàm loss function này đã chứng minh được tính hiệu quả trong các bài toán object detection. Đây là lớp bài toán có sự mất cân bằng nghiêm trọng giữa hai class positive (các bounding box có chứa object) và negative (các bounding box không chứa object). Thường thì negative có số lượng lớn hơn positive rất nhiều. Lấy ví dụ như hình \ref{fig:pos-neg}: Chỉ có 4 bounding box thuộc positive (đường viền in đậm), các trường hợp còn lại thuộc nhóm negative. \\
Do đó nếu áp dụng loss function là hàm cross entropy sẽ giảm độ chính xác khi dự báo các bounding box có chứa object. Trong bài báo Focal Loss for Dense Object Detection tác giả đã giới thiệu hàm Focal Loss có công thức như sau:
\begin{equation}
	FP(\mathbf{p}, \mathbf{q}) = - (1-q_i)^{\gamma} \log(q_i), ~~~ \text{với} ~ p_i=1
\end{equation}
Ta thấy hàm focal loss chỉ thêm nhân tử $(1-q_i)^{\gamma}$ so với công thức của balanced cross entropy. Tuy nhiên nhân tử này lại có tác dụng rất lớn trong việc điều chỉnh ảnh hưởng của nhãn lên đồng thời loss function và gradient descent. Thật vậy, xét hai trường hợp dễ dự báo (easy examples) và khó dự báo (hard examples):
\begin{itemize}
	\item Dễ dự báo: Chúng ta thấy rằng mô hình huấn luyện trên mẫu mất cân bằng thường dự báo chính xác các mẫu đa số. Những trường hợp này được gọi là dễ dự báo. Xác suất $q_i$ của của các trường hợp dễ dự báo có xu hướng cao hơn. Do đó $(1-p_t)^{\gamma}$  có xu hướng rất nhỏ và dường như không tác động lên hàm mất mát đáng kể.
	\item Khó dự báo: Trường hợp khó dự báo thì  $q_i$  là một giá trị nhỏ hơn. Do đó độ lớn tác động của nó lên loss function là $(1-p_t)^{\gamma}$ sẽ gần bằng 1. Mức độ tác động này lớn hơn rất nhiều lần so với trường hợp dễ dự báo. Cụ thể hơn, nếu trường hợp dễ dự báo có $p_i = 0.9$ và khó dự báo có $p_i = 0.1$ thì tỷ lệ chênh lệch của đóng góp vào loss function (giả sử khi $\gamma=2$) sẽ là:
 \begin{equation*}
 	\frac{(1-0.1)^2}{(1-0.9)^2} = \frac{0.9^2}{0.1^2} = 81
 \end{equation*}
Tỷ lệ này sẽ còn lớn hơn nữa nếu tăng $\gamma$ hoặc giá trị của $p_i$ đối với trường hợp dễ dự báo càng gần 1 và khó dự báo càng gần 0.

\end{itemize}

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\linewidth]{Images/dog_bike}
	\caption{Minh hoạ các lớp positive và negative trong object detection}
	\label{fig:pos-neg}
\end{figure} 




\chapter{Ứng dụng học sâu vào bài toán đếm cây trên ảnh viễn thám}
\section{Giới thiệu bài toán}
\hspace*{1cm} Cây keo là một loại cây dễ trồng, có khả năng thích nghi với các loại đất nghèo chất dinh dưỡng, ở những nơi thời tiết khắc nghiệt. Gỗ keo có rất nhiều tác dụng nên keo rất phù hợp với các dự án lâm nghiệp thương mại: là nguyên liệu sử dụng để sản xuất đồ nội thất xuất khẩu và sản xuất giấy. Cây keo ở Việt Nam ngày càng được trồng với quy mô lớn, nhằm nhanh chóng phủ xanh đồi núi trọc keo được trồng thành rừng, đem lại nguồn thu nhập lớn, tạo công ăn việc làm cho nhiều người dân. Để đáp ứng nhu cầu tiêu thụ gỗ ngày càng tăng ở trong nước và thế giới, sản lượng nông nghiệp tiêu dùng, năng suất cây trồng phải được ước tính. Việc phát hiện, kiểm soát các cây keo trên một diện tích lớn trong khu rừng giúp cho doanh nghiệp và các nhà quản lý biết được tình trạng hiện tại của các cây keo, có thể trồng thêm được cây nào vào những khu đất còn trống. Tuy nhiên việc thu thập thủ công dữ liệu cây trồng để lưu trữ trong hồ sơ trên một diện tích đất lớn là không khả thi đối với con người, gây tốn kém tiền bạc và dễ bị ảnh hưởng bởi lỗi của con người. \\
\hspace*{1cm} Một tiềm năng lớn để giải quyết vấn đề này là sử dụng các hình ảnh vệ tinh kết hợp với các thuật toán thị giác máy tính để phát hiện quản lý những cây trồng này. Sử dụng hình ảnh vệ tinh để khảo sát sẽ giảm bớt một phần công việc của con người khi phải trực tiếp đến khu vực đó tiến hành theo dõi, phân tích. Các bức ảnh viễn thãm được chụp lại với kích thước lớn, có thể bao quát một khu diện tích đất trồng rộng giúp tiết kiệm thời gian điều tra khảo sát, từ đó giúp các nhà quản lý của doanh nghiệp có thể nắm bắt tình hình kịp thời, lập kệ hoạch sản xuất phù hợp \\
\hspace*{1cm}Để việc giải quyết bài toán này trở nên đơn giản và tự động hơn, trong đồ án này, em trình bày về việc sử dụng các phương pháp Deep Learning để tiến hành phân tích và phát hiện các cây trong bức ảnh viễn thám
\begin{figure}[!h]
    \centering
    \begin{subfigure}[!h]{0.45\textwidth}
    \includegraphics[width=\linewidth]{Images/Img_RSKA003603_0_r2560_c3200_2}
        \subcaption[short for lof]{Ảnh ban đầu}
%        \rule{\linewidth}{3cm}
        \label{fig:subfig1}
    \end{subfigure}%
    \quad
    \begin{subfigure}[!h]{0.45\textwidth}
    \includegraphics[width=\linewidth]{Images/Img_RSKA003603_0_r2560_c3200}
        \subcaption[]{Kết quả sau khi nhận dạng đối tượng trong ảnh}
%        \rule{\linewidth}{3cm}
        \label{fig:subfig2}
    \end{subfigure}
\end{figure}


\section{Mô hình hoá bài toán và thiết kế dữ liệu luyện}
\begin{enumerate}[label= \textit{\alph*)}]
\item \textit{Mô hình hóa bài toán} \\
\textbf{Input:} \\
Ảnh viễn thám được lưu ở định dạng .tif, có kích thước lớn (từ vài chục MB đến GB) \\ 
\textbf{Tiền xử lý:} \\
Ảnh đầu vào sẽ được chia nhỏ thành các ảnh có kích thước $256 \times 256$ (bằng với kích thước trong tập ảnh training),  với tỉ lệ \textit{overlap} giữa các tấm ảnh là $0.5$ \\
\textbf{Output:} \\
Các file định dạng .txt chứa các bounding box mà mô hình đoán đó là vật thể trong bức ảnh đó.  
Vì mục tiêu của bài toán là nhận dạng và đếm số cây trên một bức ảnh lớn,  nên ta sẽ ghép các tấm ảnh nhỏ ở trên lại để đưa về hình ảnh gốc của chúng.  Vì trong quá trình tiền xử lý, ta thiết lập overlap $= 0.5$ nên việc các bounding box của các bức ảnh gần nhau sẽ đè lên nhau,  gây ra hiện tượng trùng lặp.  Để giải quyết vấn đề này,  các bounding box thu được trên toàn bộ các ảnh đầu tiên sẽ được đưa về toạ độ địa lý, sau đó sử dụng thuật toán NMS để loại bỏ các bounding box trùng nhau (nếu như giá trị IOU của hai bounding box này vượt quá một ngưỡng (trong mô hình này em lấy ngưỡng $=0.3$)).  Kết quả sau khi thực hiện thuật toán NMS sẽ 


\item \textit{Thiết kế dữ liệu luyện mạng} \\
Dữ liệu sử dụng trong bài toán này được cung cấp bởi thầy giáo hướng dẫn TS.  Lê Hải Hà.  Dữ liệu bao gồm 2 thư mục,  nỗi thư mực gồm một vài ảnh ở định dạng GeoTif và các \textbf{shapefile} chứa toạ độ các bounding box trong các bức ảnh đó
 \begin{figure}[!htb]
	\centering
	\includegraphics[width=1\linewidth]{Images/data_folder}
	\caption{Dữ liệu sử dụng trong bài toán}
	\label{fig:data_folder}
\end{figure}

\newpage
 \begin{figure}[!htb]
	\centering
	\includegraphics[width=1\linewidth]{Images/qgisa}
	\caption{Minh hoạ ảnh viễn thám với phần mềm QGIS} 
	\label{fig:1a}
\end{figure}


 \begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\linewidth]{Images/qgisb}
	\caption{Toạ độ các bounding box được gán nhãn trên ảnh,  biểu thị bởi các hình tròn màu vàng}
	\label{fig:1b}
\end{figure}


%\begin{figure}
%  \begin{subfigure}{0.6\textwidth}
%    \includegraphics[width=\linewidth]{Images/qgisa}
%    \caption{} \label{fig:1a}
%  \end{subfigure}%
%  \hspace*{\fill}   % maximize separation between the subfigures
%  \begin{subfigure}{0.5\textwidth}
%    \includegraphics[width=\linewidth]{Images/qgisb}
%    \caption{} \label{fig:1b}
%  \end{subfigure}%
%  \hspace*{\fill}   % maximizeseparation between the subfigures

%
%\caption{Minh hoạ ảnh viễn thám với phần mềm QGIS (hình \ref{fig:1a}),  toạ độ các bounding box trên ảnh,  biểu thị bởi các hình tròn màu vàng (hình \ref{fig:1b})} \label{fig:1}
%\end{figure}
\newpage
Các ảnh trong bộ dữ liệu ở trên có 2 loại kích thước : $3141 \times 1885$ và $3874 \times 3100$.  Để thuận tiện cho việc training mô hình,  ta sẽ chia nhỏ mỗi ảnh thành nhiều ảnh con có kích thước $256 \times 256$ với giá trị $overlap = 0.5$.  Để lấy bounding box trong từng bức ảnh được cắt ra,  đầu tiên ta tiến hành đọc file \textit{shapefile} của ảnh đó bằng thư viện \textit{geopandas} (hình \ref{fig:polygon_ori}): 

 \begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{Images/polygon_ori}
	\caption{Đọc file shapefile với thư viện geopandas}
	\label{fig:polygon_ori}
\end{figure}
Vì toạ độ các câu được đánh dấu theo hình tròn nên ta cần chuyển định dạng hình tròn về bounding box (hình \ref{fig:polygon_bounds})

 \begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{Images/polygon_bounds}
	\caption{Toạ độ 4 đỉnh của một polygon}
	\label{fig:polygon_bounds}
\end{figure}


%\begin{figure}
%  \begin{subfigure}{0.5\textwidth}
%    \includegraphics[width=\linewidth]{Images/split1}
%    \caption{} \label{fig:1a}
%  \end{subfigure}%
%  \hspace*{\fill}   % maximize separation between the subfigures
%  \begin{subfigure}{0.5\textwidth}
%    \includegraphics[width=\linewidth]{Images/split2}
%    \caption{} \label{fig:1b}
%  \end{subfigure}%
%  \hspace*{\fill}   % maximizeseparation between the subfigures
%%    \begin{subfigure}{0.4\textwidth}
%%    \includegraphics[width=\linewidth]{Images/split3}
%%    \caption{} \label{fig:1b}
%%  \end{subfigure}%
%\caption{Minh hoạ ảnh viễn thám với phần mềm QGIS (hình \ref{fig:1a}),  toạ độ các bounding box trên ảnh,  biểu thị bởi các hình tròn màu vàng (hình \ref{fig:1b})} \label{fig:1}
%\end{figure}


%
% \begin{figure}[!h]
%	\centering
%	\includegraphics[width=0.5\linewidth]{Images/split3}
%	\caption{Toạ độ 4 đỉnh của một polygon}
%	\label{fig:polygon_bounds}
%\end{figure}
\newpage
Tuy nhiên trong hình trên, mỗi điểm là một toạ độ trên một mặt phẳng địa lý,  ta phải chuyển các toạ độ $x,y$ về toạ độ cột,  hàng tương ứng.  Một vài kết quả thu được sau khi tiến hành cắt ảnh và lấy bounding box tương ứng cho mỗi bức ảnh đó: 
\newpage
\begin{figure}[!h]
    \centering
    \begin{subfigure}[!h]{0.45\textwidth}
    \includegraphics[width=\linewidth]{Images/original/Img_RSKA003603_0_r000_c256}
        \subcaption[short for lof]{}
%        \rule{\linewidth}{3cm}
        \label{fig:subfig1}
    \end{subfigure}%
    \quad
    \begin{subfigure}[!h]{0.45\textwidth}
    \includegraphics[width=\linewidth]{Images/original/Img_RSKA003603_0_r256_c3328}
        \subcaption[short for lof]{}
%        \rule{\linewidth}{3cm}
        \label{fig:subfig2}
    \end{subfigure}
 
    \begin{subfigure}[!h]{0.5\textwidth}
    	\includegraphics[width=\linewidth]{Images/original/Img_RSKA014702_3_r1664_c384}
        \subcaption[short for lof]{}
%        \rule{\linewidth}{3cm}
        \label{fig:subfig3}
    \end{subfigure}
    \caption[Optional caption for list of figures]{Một vài ảnh trọng bộ dữ liệu được tạo ra}
\end{figure}
\end{enumerate}
Kết quả sau khi cắt ảnh ra ta thu được bộ dữ liệu hơn 5000 ảnh kích thước $256 \times 256$ cùng với toạ độ các bounding box chứa đối tượng trong từng ảnh đó
\section{Huấn luyện mạng neural}
Chúng ta sẽ tiến hành huấn luyện mô hình dựa trên bộ dữ liệu được tạo ra ở trên.  Mô hình mà em sử dụng trong bài toán này là RetinaNet; trong đó backbone sử dụng ở đây là ResNet50 được pretrained trên bộ dữ liệu nổi tiếng ImageNet phục vụ cho việc trích xuất các đặc trưng trong ảnh
\section{Xây dựng chương trình}
Chương trình được xây dựng trên nền tảng ngôn ngữ Python (phiên bản 3.6.9).. Cấu trúc thư mục của chương trình như hình 4.2 dưới đây: 
 \begin{figure}[!h]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/tree_DATN}
	\caption{Cấu trúc chương trình}
	\label{fig:tree_DATN}
\end{figure}
Dưới đây ta sẽ tìm hiểu kỹ chức năng các thư mục và file trên:
\begin{itemize}
	\item $/$data\_tiff: Thư mục này chứa các ảnh viễn thám định dạng GeoTiff cùng với các file ".shp" chứa các thông tin về đặc dạng hình học của đối tượng có trong ảnh
	\item $/$retinanet: Thư mục xây dưng mô hình RetinaNet: khởi tạo model,  hàm loss, DataLoader để đưa dữ liệu vào mô hình
	\item $/$checkpoint: Thư mục này lưu lại các trọng số trong mạng sau mỗi lần duyệt qua toàn bộ dataset, phục vụ cho việc đánh giá và dự đoán model. Các file trong thư mục này đều có định dạng: *.pt
	\item /raster\_split: Thư mục này dùng để chuyển các ảnh vệ tinh định dạng GeoTiff về định dạng .png đồng thời tạo một bộ dataset gồm các ảnh png và file .txt chứa vị trí các đối tượng trong ảnh.
	\item /lib: Thư mục này xây dựng các metrics trong việc đánh giá một mô hình object detection: Precision, Recall, Average Precision, F1-score.
	\item demo.py: Kết hợp với các file trong thư mục /demo thực hiện nhiệm vụ testing: Đầu tiên sẽ load file weight trong thư mục checkpoint, đọc dữ liệu ảnh GeoTiff đầu vào và đưa vào trong model. Kết quả sau đó được lưu lại và xử lý để xuất ra shapefile
	\item train.py: File này có nhiệm vụ chạy mô hình RetinaNet và lưu trọng số của mô hình vào thư mục /checkpoint sau mỗi lần lặp trên toàn  bộ dataset. Thao tác này chỉ cần thực hiện một lần duy nhất.
\end{itemize}

\section{Kết quả huấn luyện}


\chapter{Cài đặt chương trình và đánh giá kết quả}

\section{Môi trường cài đặt chương trình và các yêu cầu liên quan}
\subsection{Môi trường cài đặt chương trình}
Chương trình được cài đặt trên ngôn ngữ Python (phiên bản 3.6.9) và được thử nghiệm trên hệ điều hành máy ảo của google colab sử dụng chip 4 cores , bộ nhớ RAM 24GB,  card đồ hoạ NVIDIA Tesla V100 16GB (hình \ref{fig:nvidia-smi})
 \begin{figure}[!h]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/nvidia-smi}
	\caption{Cấu hình máy tính sử dụng cho bài toán}
	\label{fig:nvidia-smi}
\end{figure}
\subsection{Các yêu cầu liên quan}
\begin{itemize}
	\item Ngôn ngữ sử dụng: Python phiên bản 3.6.9
	\item IDE sử dụng: Visual Studio Code
	\item Các thư viện Python sử dụng trong chương trình được viết trong file " requirements.txt"
\end{itemize}



\section{Dữ liệu đầu vào}
Vì dữ liệu em được cung cấp bị hạn chế, nên trong bộ dữ liệu ban đầu em lấy ra 1 ảnh chưa được xử lý trong tập train và validation (hình \ref{fig:demo}) để làm dữ liệu đầu vào cho mô hình nhận diện vật thể đã xây dựng ở chương trước.  Lý do của việc lấy một ảnh kích thước lớn để dự đoán là vì theo yêu cầu của khách hàng,  họ cần đánh giá kết quả mô hình trên toàn bộ diện tích đất của họ chứ không phải các tấm ảnh kích thước $256 \times 256$ được cắt nhỏ từ tấm ảnh ban đầu.
 \begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{Images/demo}
	\caption{Dữ liệu đầu vào để phát hiện vật thể}
	\label{fig:demo}
\end{figure}

\section{Kết quả mô hình}
Thời gian chạy chương trình cho ảnh trên: $43$ giây \\
Hình ảnh kết quả được lưu lại ở định dạng shapefile: 	(hình \ref{fig:shpfile_demo_result})
 \begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\linewidth]{Images/shpfile_demo_result}
	\caption{Kết quả mô hình: thư mục chứa các shapfile}
	\label{fig:shpfile_demo_result}
\end{figure}




Ta sử dụng file shapefile ở trên để mô phỏng kết quả trên phần mềm QGIS (hình \ref{fig:demo_result}): 
 \begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{Images/demo_result}
	\caption{Kết quả mô hình: các ô vuông màu xanh là kết quả phát hiện của mô hình; chấm màu cam là các cây được gán nhãn sẵn. }
	\label{fig:demo_result}
\end{figure}


\section{Đánh giá kết quả}
Hình \ref{fig:demo_result} chỉ cho ta một cách khái quát về kết quả mô hình khi so sánh với các bounding box ban đầu.  Để đánh giá kết quả của mô hình,  trước tiên ta định nghĩa: Hai bounding box cùng phát hiện một đối tượng nếu giá trị IOU của chúng $\geq 0.3$. Khi đó,  bounding box được dự đoán từ mô hình sẽ được coi là một \textit{True positive},  ngược lại sẽ là \textit{False positive}.  Trong trường hợp có nhiều hơn 1 bounding box cùng dự đoán một đối tượng,  ta sẽ lấy bounding box có confidence (độ tin cậy) cao nhất làm TP, các bounding box còn lại sẽ là FP.  Để so sánh một cách khách quan, em sử dụng cùng một tập các thang đo phổ biến là Precision, Recall và F1-score.  Trong đó, Precision là tỉ lệ giữa số cây được phát hiện chính xác trên tổng số cây được phát hiện. Recall là tỉ lệ giữa số cây được 
phát hiện đúng trên tổng số cây thực sự trong ảnh.
F1-score là trung bình điều hòa giữa hai giá trị Precision và Recall. Các tham
số trên được biểu diễn trong các biểu thức:
\begin{equation*}
	Precision = \frac{TP}{TP + FP}
\end{equation*}

\begin{equation*}
	Recall = \frac{TP}{TP + FN}
\end{equation*}

\begin{equation*}
	F1 - score = \frac{2 \times Precision \times Recall}{Precision + Recall}
\end{equation*}
%Lần lượt tính toán các bounding box và áp dụng công thức của Precision,  Recall,  F1,  ta thu được các thông số cho việc đánh giá kết quả như sau: 
\begin{figure}[!h]
	\centering
  	\includegraphics[width=.8\linewidth]{Images/f1-score} %example-image-a
 	 \caption{Đánh giá kết quả}
 	 \label{fig:sub1}
\end{figure}

\section{Định hướng phát triển trong tương lai}
Từ kết quả thực nghiệm cho thấy, chương trình đã có những thành công nhất định. Song bên cạnh đó cũng còn khá nhiều nhược điểm cần cải tiến. Trong quá trình hoàn thành đồ án em đã đầu tư nhiều thời gian và công sức với bài toán này, và nhận thấy đây là bài toán có khả năng phát triển cao hơn nữa . Em xin đưa ra một số hướng phát triển tiếp theo cho bài toán như sau:
\begin{itemize}
	\item Phát triển và cài đặt giao diện chương trình để có thể dễ dàng chạy và sử dụng hơn. 
	\item Sử dụng thêm một vài phép xử lý ảnh để tăng cường dữ liệu phục vụ cho việc training 
	\item Cải tiến, thay đổi các thuật toán để loại bỏ các bounding box trùng nhau nhanh hơn khi sử dụng thuật toán NMS; giúp chương trình chạy tối ưu hơn
\end{itemize}




%\begin{figure}
%\centering
%\begin{subfigure}{.5\textwidth}
%  \centering
%  \includegraphics[width=.4\linewidth]{example-image-a}
%  \caption{A subfigure}
%  \label{fig:sub1}
%\end{subfigure}%
%\begin{subfigure}{.5\textwidth}
%  \centering
%  \includegraphics[width=.4\linewidth]{example-image-a}
%  \caption{A subfigure}
%  \label{fig:sub2}
%\end{subfigure}
%\caption{A figure with two subfigures}
%\label{fig:test}
%\end{figure}

%\chapter*{Tài liệu tham khảo}
%\addcontentsline{toc}{chapter}{Tài liệu tham khảo}
%\mdseries 
%-------------------------------------------------------------------------------------
\newpage
\addcontentsline{toc}{section}{Tài liệu tham khảo}
\begin{thebibliography}{9}
	\bibitem{mlcoban} 
	Vũ Hữu Tiệp. 
	\textit{Machine Learning cơ bản}. machinelearningcoban.com/ebook, 2018.
	
	\bibitem{dlpython} 
	François Chollet. 
	\textit{Deep Learning with Python}. \textcopyright 2018 Manning Publications Co.
	
	\bibitem{dlpytorch} 
	Eli Stevens, Luca Antiga. 
	\textit{Deep Learning with PyTorch, Essential Excerpts}. \textcopyright 2019 Manning Publications Co.
	
	\bibitem{d2l} 
	Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. 
	\textit{Dive into Deep Learning}. Release 0.14.3, 8/2020
	
	\bibitem{coco} 
	Tsung-Yi Lin,
	James Hays,
	Michael Maire Serge Belongie,
	Pietro Perona Deva Ramanan,
	Lubomir Bourdev Ross Girshick,
	C. Lawrence Zitnick Piotr Dollár. 
	\textit{Microsoft COCO: Common Objects in Context}.  
	arXiv:1405.0312.
	
	\bibitem{fastrcnn} 
	Ross Girshick. 
	\textit{Fast R-CNN}.  
	arXiv:1504.08083.
	
	\bibitem{fasterrcnn} 
	Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 
	\textit{Faster R-CNN: Towards Real-Time Object
		Detection with Region Proposal Networks}.  
	arXiv:1506.01497.
		
	\bibitem{cpoints} 
	Tsung{-}Yi Lin,
               Priya Goyal,
               Ross B. Girshick,
               Kaiming He and
               Piotr Doll{\'{a}}r.
	\textit{Focal Loss for Dense Object Detection}.  
	arXiv:1708.02002.
	
	\bibitem{cfpn} 
	Tsung{-}Yi Lin,
               Piotr Doll{\'{a}}r,
               Ross B. Girshick,
               Kaiming He,
               Bharath Hariharan and
               Serge J. Belongie.
	\textit{Feature Pyramid Networks for Object Detection}.  
	arXiv:1612.03144.
	
\end{thebibliography}

\end{document}